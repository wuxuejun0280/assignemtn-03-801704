# Part-1 Design for streaming analytics
## 1. Description of dataset, streaming analytics and batch analytics

The dataset we pick is gonna be about the yellow taxi in New York city. It contains information about the vender id, drop off location, pick up location, drop of timestamp, pick up timestamp, distance, price, etc. The detailed description and sample dataset is in the `data/` folder. In the design of the platform we assume not all taxies can upload the data of trip right after the trip end. Great amount of data may delay for days because many taxi have to upload data stored locally manually. 

In the streaming analytics we will perform on the dataset will be recording the amount of pick up per location every one hour. We will use the process time to divide the message into different partitions and find out the interval of pickup time(event time). Then we will return the amount of pick up per hour per location. For example, between 17:00 and 18:00 the interval of pick up time is 16:00 to 18:00. Then we will count the amount of pick up per location from 16:00 to 17:00 and from 17:00 to 18:00 and return to user. 

In the batch analytics we will find out the average amount of pick up at that specific time in a year per location. For example, the batch should output data with this schema:`month, day, week, start time, end time, average pick up`. Also we can find out the average distance and average expense per pick up at specific time and location. The reason we do this in batch analytics is because the taxi data doesn't necessarily arrive right after the drop off. So we have to calculate the average everytime the stream analytics indicate more data in that timeinterval is processed and we have to calculate the average of that time interval again. 

## 2. Regarding the key of data streams and delivery guarantees.
First, the analytics should handle keyed data streams. It's because the only with keyed window flink can execute the window computation in parallel. If we use non-keyed window, all the data will come in as one data stream in one task which can increase the latency of streaming anlysis. 

Second, the best delivery guarantees should be exactly once to maintain the integrity of data. However since some data is sent through network from taxies, a good network condition cannot be garanteed. If we cannot garantee the data to be processed exactly once, I think we should guarantee the data to be delivered at least once. Since some region has bad network condition, the chance the data fails to reach message queue is higher. If we use the at-most-once delivery guarantee, the amount pick up at certain location can be biased because lots of data is lost. For a taxi company this outcome will decrease the revenue and bring unconvient to people in certain location. So we will prefer to choose either exactly-once or at-least-once delivery guarantees depend on situation. If it's not possible to use exactly-once delivery guarantees, we will use at-least-once delivery guarantees. 

## 3. Regarding choice of time and window
First, the time we are gonna use is the process time. Idealy we will use the event time but since the delay of data can vary from minutes to days, it is hard to actually implement this. In flink there is a maximum delay for event time. If we set the delay very large, the streaming data won't be able to return on time. Thus here we are gonna use the process time to partition the data into different window and perform analysis on event time. 

Second, we are gonna use tumbling windows for analytics. In stream analysis we are tring to get the amount of pick up every one hour so we are choosing tumbling windows. If we need a average in a time period, we can use sliding window. However in this design that's the job of batch analysis. The seesion windows is not really applicable here because there won't be periodical gap between cluster of data. 

## 4. About the performance matices. 
The throughput is the most important performance matices in this case. That's because we need to make sure the queue of message borker won't keep increase and lost data from the taxi. The latency is relatively less important since the customer probably can accept delay within 10 seconds. The possible use of the data is to manage the taxi distribution around the city and a 10 second delay won't have big impact on the customer since the relocation of taxi probably takes more than 10 minutes. Also because we cannot ensure the network condition, the latency can depend more on the network condition which is impossible to improve through the design of the platform. 

## 5. The design of stream analytics
![](https://github.com/wuxuejun0280/assignemtn-03-801704/blob/master/reports/assignement3.png "design")
The datasource here can be a app of the taxi terminal or a client somewhere taxi can upload data stored locally in terminal. The message broker here is rabbitmq. Then the *clientstreamapp* will reterive the data from *input queue*, perform the analysis and send the data to the *output queue*. The *stream computing service* give api for customer to submit the *clientstreamapp*. It is also in charge of the lifecycle of each *clientstreamapp*. We could have multiple *clientstreamapp* in parallel with *input queue* and *output queue* for each of them.Here we use Rabbitmq because it make sure all the data reterived must be in order. Also the queue persist the message before it is processed, which gives the system more stability. We are gonna use Flink. The main reason is because the learning curve for spark is more steep which will bring inconvenience to customer when they are developing *clientstreamapp*. Also it seems Flink integrate batch analytics and stream analytics much better through better memory management. 
